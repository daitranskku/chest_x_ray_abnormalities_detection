{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lesser-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSPECT TRAINED MODEL\n",
    "# Load libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "from skimage import exposure\n",
    "from sklearn import preprocessing\n",
    "from skimage.measure import find_contours\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instant-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_DIR = \"/media/daitran/Data/Kaggle/VinBigData\"\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "TRAIN_CSV_DIR = os.path.join(DATA_DIR, \"train.csv\")\n",
    "SS_CSV_DIR = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "PREPROCESSED_TRAINING_IMAGE_FOLDER = '/home/daitran/Desktop/research/kaggle/VinBigData/train/512_jpg/'\n",
    "resized_test_folder = '/home/daitran/Desktop/research/kaggle/VinBigData/test/'\n",
    "\n",
    "orin_df = pd.read_csv(TRAIN_CSV_DIR)\n",
    "orin_df = orin_df.query('class_id != 14')\n",
    "\n",
    "# Load training dataframe .csv\n",
    "training_df = pd.read_csv('/home/daitran/Desktop/git/chest_x_ray_abnormalities_detection/MaskRCNN_implementation/sample_df.csv', converters ={'EncodedPixels': eval, 'CategoryId': eval})\n",
    "samples_df = training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baking-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATIONS\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "NUM_CATS = 14\n",
    "IMAGE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proof-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Config \n",
    "class DiagnosticConfig(Config):\n",
    "    NAME = \"Diagnostic\"\n",
    "    NUM_CLASSES = NUM_CATS + 1 # +1 for the background class\n",
    "\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 10 #That is the maximum with the memory available on kernels\n",
    "\n",
    "    BACKBONE = 'resnet50'\n",
    "\n",
    "    IMAGE_MIN_DIM = IMAGE_SIZE\n",
    "    IMAGE_MAX_DIM = IMAGE_SIZE\n",
    "    IMAGE_RESIZE_MODE = 'none'\n",
    "\n",
    "    POST_NMS_ROIS_TRAINING = 250\n",
    "    POST_NMS_ROIS_INFERENCE = 150\n",
    "    MAX_GROUNDTRUTH_INSTANCES = 5\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "    BACKBONESHAPE = (8, 16, 24, 32, 48)\n",
    "    RPN_ANCHOR_SCALES = (8,16,24,32,48)\n",
    "    ROI_POSITIVE_RATIO = 0.33\n",
    "    DETECTION_MAX_INSTANCES = 300\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7\n",
    "\n",
    "\n",
    "    STEPS_PER_EPOCH = int(len(samples_df)*0.8/IMAGES_PER_GPU)\n",
    "    VALIDATION_STEPS = int(len(samples_df)/IMAGES_PER_GPU)-int(len(samples_df)*0.9/IMAGES_PER_GPU)\n",
    "\n",
    "config = DiagnosticConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "partial-difference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONESHAPE                  (8, 16, 24, 32, 48)\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        300\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_META_SIZE                27\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              none\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GROUNDTRUTH_INSTANCES      5\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           Diagnostic\n",
      "NUM_CLASSES                    15\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        150\n",
      "POST_NMS_ROIS_TRAINING         250\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 24, 32, 48)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                2887\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               361\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Inference Config\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quarterly-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract class names\n",
    "category_list = orin_df.class_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "linear-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Mask RCNN formart dataset\n",
    "class DiagnosticDataset(utils.Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(self)\n",
    "\n",
    "        # Add classes\n",
    "        for i, name in enumerate(category_list):\n",
    "            self.add_class(\"diagnostic\", i+1, name)\n",
    "\n",
    "        # Add images\n",
    "        for i, row in df.iterrows():\n",
    "            self.add_image(\"diagnostic\",\n",
    "                           image_id=row.name,\n",
    "                           path= PREPROCESSED_TRAINING_IMAGE_FOLDER+str(row.image_id)+\".jpg\",\n",
    "                           labels=row['CategoryId'],\n",
    "                           annotations=row['EncodedPixels'],\n",
    "                           height=row['Height'], width=row['Width'],\n",
    "                           img_org_id = row.image_id)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path'], [category_list[int(x)] for x in info['labels']]\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "\n",
    "        return cv2.imread(self.image_info[image_id]['path'])\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "\n",
    "        mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE, len(info['annotations'])), dtype=np.uint8)\n",
    "        labels = []\n",
    "        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n",
    "            sub_mask = np.full(info['height']*info['width'], 0, dtype=np.uint8)\n",
    "\n",
    "            annotation = [int(x) for x in annotation.split(' ')]\n",
    "\n",
    "            for i, start_pixel in enumerate(annotation[::2]):\n",
    "                sub_mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n",
    "\n",
    "            sub_mask = sub_mask.reshape((info['height'], info['width']), order='F')\n",
    "            sub_mask = cv2.resize(sub_mask, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            mask[:, :, m] = sub_mask\n",
    "            labels.append(int(label)+1)\n",
    "        return mask, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "primary-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# Split with train = 80% samples and val = 10% and test = 10%\n",
    "training_percentage = 0.8\n",
    "\n",
    "training_set_size = int(training_percentage*len(samples_df))\n",
    "validation_set_size = int((0.9-training_percentage)*len(samples_df))\n",
    "test_set_size = int((0.9-training_percentage)*len(samples_df))\n",
    "\n",
    "train_dataset = DiagnosticDataset(samples_df[:training_set_size])\n",
    "train_dataset.prepare()\n",
    "\n",
    "valid_dataset = DiagnosticDataset(samples_df[training_set_size:training_set_size+validation_set_size])\n",
    "valid_dataset.prepare()\n",
    "\n",
    "test_dataset = DiagnosticDataset(samples_df[training_set_size + validation_set_size:])\n",
    "test_dataset.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "standing-punch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 3609\n",
      "Classes: ['BG', 'Cardiomegaly', 'Aortic enlargement', 'Pleural thickening', 'ILD', 'Nodule/Mass', 'Pulmonary fibrosis', 'Lung Opacity', 'Atelectasis', 'Other lesion', 'Infiltration', 'Pleural effusion', 'Calcification', 'Consolidation', 'Pneumothorax']\n"
     ]
    }
   ],
   "source": [
    "# Show validation dataset information\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(valid_dataset.image_ids), valid_dataset.class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fundamental-detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "BG\n",
      "1\n",
      "Cardiomegaly\n",
      "2\n",
      "Aortic enlargement\n",
      "3\n",
      "Pleural thickening\n",
      "4\n",
      "ILD\n",
      "5\n",
      "Nodule/Mass\n",
      "6\n",
      "Pulmonary fibrosis\n",
      "7\n",
      "Lung Opacity\n",
      "8\n",
      "Atelectasis\n",
      "9\n",
      "Other lesion\n",
      "10\n",
      "Infiltration\n",
      "11\n",
      "Pleural effusion\n",
      "12\n",
      "Calcification\n",
      "13\n",
      "Consolidation\n",
      "14\n",
      "Pneumothorax\n"
     ]
    }
   ],
   "source": [
    "k= 0\n",
    "for class_name in valid_dataset.class_names:\n",
    "    print(k)\n",
    "    print(class_name)\n",
    "    k +=1\n",
    "# print(valid_dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-physics",
   "metadata": {},
   "source": [
    "## BG = Background!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-short",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/daitran/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Call model\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=\"\",\n",
    "                              config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained weight\n",
    "model_path = '/home/daitran/Desktop/git/chest_x_ray_abnormalities_detection/MaskRCNN_implementation/server_weights/mask_rcnn_diagnostic_0024.h5'\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original\n",
    "def display_test_result(dataset):\n",
    "    image_id = random.choice(dataset.image_ids)\n",
    "#     print(image_id)\n",
    "#     print(dataset.class_names)\n",
    "\n",
    "    # Display original Dicom\n",
    "\n",
    "    # plot_bbox(img_id = dataset.image_info[image_id]['img_org_id'])\n",
    "\n",
    "    # Display original in training form\n",
    "\n",
    "    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset, config, \n",
    "                               image_id, use_mini_mask=False)\n",
    "\n",
    "#     log(\"original_image\", original_image)\n",
    "#     log(\"image_meta\", image_meta)\n",
    "#     log(\"gt_class_id\", gt_class_id)\n",
    "#     log(\"gt_bbox\", gt_bbox)\n",
    "#     log(\"gt_mask\", gt_mask)\n",
    "    \n",
    "    print('GT')\n",
    "    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                                dataset.class_names, figsize=(5, 5))\n",
    "    print(gt_class_id)\n",
    "    \n",
    "    # Display test prediction\n",
    "\n",
    "    results = model.detect([original_image], verbose=0)\n",
    "    r = results[0]\n",
    "    \n",
    "    print('Predict')\n",
    "    print(r['class_ids'])\n",
    "    print(r['scores'])\n",
    "    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                dataset.class_names, r['scores'], figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    display_test_result(dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-lesbian",
   "metadata": {},
   "source": [
    "## Generate submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom2array(path, voi_lut=True, fix_monochrome=True):\n",
    "    dicom = pydicom.read_file(path)\n",
    "    # VOI LUT (if available by DICOM device) is used to\n",
    "    # transform raw DICOM data to \"human-friendly\" view\n",
    "    if voi_lut:\n",
    "        data = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "    else:\n",
    "        data = dicom.pixel_array\n",
    "    # depending on this value, X-ray may look inverted - fix that:\n",
    "    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        data = np.amax(data) - data\n",
    "\n",
    "    data = data - np.min(data)\n",
    "    data = data / np.max(data)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes_dict = {\"0\":14,\"1\":3,\"2\":0,\"3\":11,\"4\":5,\"5\":8,\"6\":13,\"7\":7,\n",
    "                         \"8\":1,\"9\":9,\"10\":6,\"11\":10,\"12\":2,\"13\":4,\"14\":12}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_anomalies(dicom_image):\n",
    "\n",
    "    image_dimensions = dicom_image.shape\n",
    "\n",
    "    resized_img = cv2.resize(dicom_image, (IMAGE_SIZE,IMAGE_SIZE), interpolation = cv2.INTER_AREA)\n",
    "    saved_filename = resized_test_folder+\"temp_image.jpg\"\n",
    "    \n",
    "    cv2.imwrite(saved_filename, resized_img) \n",
    "    img = cv2.imread(saved_filename)\n",
    "\n",
    "    result = model.detect([img])\n",
    "    r = result[0]\n",
    "    \n",
    "    if r['masks'].size > 0:\n",
    "        masks = np.zeros((img.shape[0], img.shape[1], r['masks'].shape[-1]), dtype=np.uint8)\n",
    "        for m in range(r['masks'].shape[-1]):\n",
    "            masks[:, :, m] = cv2.resize(r['masks'][:, :, m].astype('uint8'), \n",
    "                                        (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        y_scale = image_dimensions[0]/IMAGE_SIZE\n",
    "        x_scale = image_dimensions[1]/IMAGE_SIZE\n",
    "        rois = (r['rois'] * [y_scale, x_scale, y_scale, x_scale]).astype(int)\n",
    "        \n",
    "#         masks, rois = refine_masks(masks, rois)\n",
    "    else:\n",
    "        masks, rois = r['masks'], r['rois']\n",
    "        \n",
    "    return rois, r['class_ids'], r['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "test_file_list = os.listdir(TEST_DIR)\n",
    "\n",
    "# Convert predict to original class id\n",
    "# selected_classes_dict = {\"0\":14,\"1\":2,\"2\":0,\"3\":11,\"4\":5,\"5\":8,\"6\":13,\"7\":7,\n",
    "#                          \"8\":1,\"9\":9,\"10\":6,\"11\":10,\"12\":2,\"13\":4,\"14\":12}\n",
    "\n",
    "for image_file_name in tqdm(test_file_list[:2]):\n",
    "    \n",
    "    dicom_image = dicom2array(TEST_DIR + '/' + image_file_name)\n",
    "    image_dimensions = dicom_image.shape\n",
    "    \n",
    "    # extracrt results\n",
    "    bbox_list, class_list, confidence_list = find_anomalies(dicom_image)\n",
    "    \n",
    "#     print(class_list)\n",
    "#     # convert from single-channel grayscale to 3-channel RGB\n",
    "#     img = np.stack([dicom_image] * 3, axis=2)\n",
    "#     resized_img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "#     # visualize\n",
    "#     result = model.detect([resized_img])\n",
    "#     r = result[0]\n",
    "    \n",
    "#     visualize.display_instances(resized_img, r['rois'], r['masks'], r['class_ids'], \n",
    "#                                 train_dataset.class_names, r['scores'], show_mask = True,\n",
    "#                                figsize=(5,5))\n",
    "    \n",
    "    # found abnormalities\n",
    "    \n",
    "    prediction_string = \"\"\n",
    "    \n",
    "    if len(bbox_list) > 0:\n",
    "        \n",
    "        for bbox, class_id, confidence in zip(bbox_list, class_list, confidence_list):\n",
    "            \n",
    "            # Convert to submission class id\n",
    "            for key, value in selected_classes_dict.items():\n",
    "                if class_id == int(key):\n",
    "                    class_id_correct = value\n",
    "            \n",
    "#             print(class_id_correct)\n",
    "            confidence_score = str(round(confidence,3))\n",
    "            \n",
    "#             y_scale = image_dimensions[0]/IMAGE_SIZE\n",
    "#             x_scale = image_dimensions[1]/IMAGE_SIZE\n",
    "            rescaled_bbox = (bbox * [1, 1, 1, 1]).astype(int)\n",
    "            \n",
    "    \n",
    "            #organise the bbox into xmin, ymin, xmax, ymax\n",
    "            ymin = image_dimensions[0]-rescaled_bbox[2]\n",
    "            ymax = image_dimensions[0]-rescaled_bbox[0]\n",
    "            xmin = rescaled_bbox[1]\n",
    "            xmax = rescaled_bbox[3]\n",
    "            \n",
    "            prediction_string += \"{} {} {} {} {} {} \".format(class_id_correct, confidence_score, xmin, ymin, xmax, ymax)\n",
    "            \n",
    "        results.append({\"image_id\":image_file_name.replace(\".dicom\",\"\"), \"PredictionString\":prediction_string.strip()})\n",
    "        \n",
    "    else:\n",
    "        results.append({\"image_id\":image_file_name.replace(\".dicom\",\"\"), \"PredictionString\":\"14 1 0 0 1 1\"})\n",
    "submission_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-banking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
