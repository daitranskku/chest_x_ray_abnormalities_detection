{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Compare encoded jpg image and dicom ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from random import randint\n",
    "import cv2\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "from skimage import exposure\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/media/daitran/Data/Kaggle/VinBigData\"\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "TRAIN_CSV_DIR = os.path.join(DATA_DIR, \"train.csv\")\n",
    "SS_CSV_DIR = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "PREPROCESSED_TRAINING_IMAGE_FOLDER = '/home/daitran/Desktop/research/kaggle/VinBigData/train/512_jpg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orin_df = pd.read_csv(TRAIN_CSV_DIR)\n",
    "orin_df = orin_df.query('class_id != 14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Helper functions for converting bounding boxes to the right format for Mask RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_mask(img_dimensions, x_min, y_min, x_max, y_max):\n",
    "    img_height, img_width = img_dimensions\n",
    "    img_mask = np.full((img_height,img_width),0)\n",
    "    img_mask[y_min:y_max,x_min:x_max] = 255\n",
    "    return img_mask.astype(np.float32)\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 255)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return ' '.join([str(x) for x in run_lengths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Function convert DICOM data to np.array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dicom2array(path, voi_lut=True, fix_monochrome=True):\n",
    "    dicom = pydicom.read_file(path)\n",
    "    # VOI LUT (if available by DICOM device) is used to\n",
    "    # transform raw DICOM data to \"human-friendly\" view\n",
    "    if voi_lut:\n",
    "        data = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "    else:\n",
    "        data = dicom.pixel_array\n",
    "    # depending on this value, X-ray may look inverted - fix that:\n",
    "    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        data = np.amax(data) - data\n",
    "\n",
    "    data = data - np.min(data)\n",
    "    data = data / np.max(data)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load img and encoded label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_df = pd.read_csv('/home/daitran/Desktop/git/chest_x_ray_abnormalities_detection/MaskRCNN_implementation/sample_df.csv', converters ={'EncodedPixels': eval, 'CategoryId': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implement Mask RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CATS = 14\n",
    "IMAGE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DiagnosticConfig(Config):\n",
    "    NAME = \"Diagnostic\"\n",
    "    NUM_CLASSES = NUM_CATS + 1 # +1 for the background class\n",
    "\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 2 #That is the maximum with the memory available on kernels\n",
    "\n",
    "    BACKBONE = 'resnet50'\n",
    "\n",
    "    IMAGE_MIN_DIM = IMAGE_SIZE\n",
    "    IMAGE_MAX_DIM = IMAGE_SIZE\n",
    "    IMAGE_RESIZE_MODE = 'none'\n",
    "\n",
    "    POST_NMS_ROIS_TRAINING = 250\n",
    "    POST_NMS_ROIS_INFERENCE = 150\n",
    "    MAX_GROUNDTRUTH_INSTANCES = 5\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "    BACKBONESHAPE = (8, 16, 24, 32, 48)\n",
    "    RPN_ANCHOR_SCALES = (8,16,24,32,48)\n",
    "    ROI_POSITIVE_RATIO = 0.33\n",
    "    DETECTION_MAX_INSTANCES = 300\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7\n",
    "    # STEPS_PER_EPOCH should be the number of instances\n",
    "    # divided by (GPU_COUNT*IMAGES_PER_GPU), and so should VALIDATION_STEPS;\n",
    "    # however, due to the time limit, I set them so that this kernel can be run in 9 hours\n",
    "    STEPS_PER_EPOCH = int(len(samples_df)*0.9/IMAGES_PER_GPU)\n",
    "    VALIDATION_STEPS = len(samples_df)-int(len(samples_df)*0.9/IMAGES_PER_GPU)\n",
    "\n",
    "config = DiagnosticConfig()\n",
    "# config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "category_list = orin_df.class_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DiagnosticDataset(utils.Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(self)\n",
    "\n",
    "        # Add classes\n",
    "        for i, name in enumerate(category_list):\n",
    "            self.add_class(\"diagnostic\", i+1, name)\n",
    "\n",
    "        # Add images\n",
    "        for i, row in df.iterrows():\n",
    "            self.add_image(\"diagnostic\",\n",
    "                           image_id=row.name,\n",
    "                           path= PREPROCESSED_TRAINING_IMAGE_FOLDER+str(row.image_id)+\".jpg\",\n",
    "                           labels=row['CategoryId'],\n",
    "                           annotations=row['EncodedPixels'],\n",
    "                           height=row['Height'], width=row['Width'],\n",
    "                           img_org_id = row.image_id)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path'], [category_list[int(x)] for x in info['labels']]\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "\n",
    "        return cv2.imread(self.image_info[image_id]['path'])\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "\n",
    "        mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE, len(info['annotations'])), dtype=np.uint8)\n",
    "        labels = []\n",
    "        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n",
    "            sub_mask = np.full(info['height']*info['width'], 0, dtype=np.uint8)\n",
    "\n",
    "            annotation = [int(x) for x in annotation.split(' ')]\n",
    "\n",
    "            for i, start_pixel in enumerate(annotation[::2]):\n",
    "                sub_mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n",
    "\n",
    "            sub_mask = sub_mask.reshape((info['height'], info['width']), order='F')\n",
    "            sub_mask = cv2.resize(sub_mask, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            mask[:, :, m] = sub_mask\n",
    "            labels.append(int(label)+1)\n",
    "        return mask, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_percentage = 0.9\n",
    "\n",
    "training_set_size = int(training_percentage*len(samples_df))\n",
    "validation_set_size = int((1-training_percentage)*len(samples_df))\n",
    "\n",
    "train_dataset = DiagnosticDataset(samples_df[:training_set_size])\n",
    "train_dataset.prepare()\n",
    "\n",
    "valid_dataset = DiagnosticDataset(samples_df[training_set_size:training_set_size+validation_set_size])\n",
    "valid_dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bbox(img_id , bbox_df = orin_df, normalize = True):\n",
    "\n",
    "    img_ids = bbox_df['image_id'].values\n",
    "    class_ids = bbox_df['class_id'].unique()\n",
    "\n",
    "    label2color = {class_id:[randint(0,255) for i in range(3)] for class_id in class_ids}\n",
    "\n",
    "    plt.figure(figsize=(20,8))\n",
    "    sub_num =1\n",
    "\n",
    "    img_id = img_id\n",
    "\n",
    "    img_path = os.path.join(TRAIN_DIR, img_id + \".dicom\")\n",
    "    img = dicom2array(img_path)\n",
    "\n",
    "    if normalize:\n",
    "        # normalize\n",
    "        img = exposure.equalize_adapthist(img/np.max(img))\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "    # convert from single-channel grayscale to 3-channel RGB\n",
    "    img = np.stack([img] * 3, axis=2)\n",
    "\n",
    "    # add bounding boxes\n",
    "    box_coordinates = bbox_df.loc[bbox_df['image_id'] == img_id, ['x_min', 'y_min', 'x_max', 'y_max']].values\n",
    "    labels = bbox_df.loc[bbox_df['image_id'] == img_id, ['class_id']].values.squeeze()\n",
    "    if not labels.shape:\n",
    "        labels = np.expand_dims(labels, axis =0)\n",
    "\n",
    "    for label_id, box in zip(labels, box_coordinates):\n",
    "        color = label2color[label_id]\n",
    "        img_bbox = cv2.rectangle(\n",
    "            img,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color = color, thickness= 8\n",
    "        )\n",
    "        # add labels\n",
    "        cv2.putText(img_bbox, str(label_id), (int(box[0]), int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 5, (36,255,12), 5)\n",
    "\n",
    "    plt.subplot(1,3,sub_num)\n",
    "    sub_num += 1\n",
    "    plt.imshow(img_bbox, cmap = 'gray')\n",
    "    plt.title('Finding contains in image')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare encoded mask with original dicom ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daitran/Desktop/research/kaggle/VinBigData/train/512_jpg/0a072917005494298d153c01bbd8f689.jpg\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    image_jpg_id = random.choice(train_dataset.image_ids)\n",
    "    print(train_dataset.image_info[image_jpg_id]['path'])\n",
    "    \n",
    "    image = train_dataset.load_image(image_jpg_id)\n",
    "    mask, class_ids = train_dataset.load_mask(image_jpg_id)\n",
    "    \n",
    "    visualize.display_top_masks(image, mask, class_ids, train_dataset.class_names, limit=5)\n",
    "    \n",
    "#     print(train_dataset.image_info[image_id]['img_org_id'])\n",
    "    plot_bbox(img_id = train_dataset.image_info[image_jpg_id]['img_org_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
